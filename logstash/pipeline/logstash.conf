input {
  beats {
    port => 5044
    client_inactivity_timeout => 120
  }
}

filter {
  # Parse JSON message structure
  json {
    source => "message"
    skip_on_invalid_json => true
    target => "parsed"
  }

  # Promote parsed fields to top level if parsing was successful
  if [parsed] {
    mutate {
      copy => { "[parsed]" => "[@metadata][parsed_data]" }
    }
    
  if [parsed][message] {
  mutate { copy => { "[parsed][message]" => "message" } }
  }
  if [parsed][log.level] {
  mutate { copy => { "[parsed][log.level]" => "[log][level]" } }
  }
  if [parsed][correlation_id] {
    mutate { copy => { "[parsed][correlation_id]" => "[correlation][id]" } }
  }
    
    # Copy application-specific fields from the parsed JSON
  ruby {
    code => '
      parsed = event.get("parsed")
      if parsed.is_a?(Hash)
        # Check for prospect object and its sub-fields
        if parsed["prospect"].is_a?(Hash)
          event.set("prospect_email", parsed["prospect"]["email"]) if parsed["prospect"]["email"]
          event.set("subject", parsed["prospect"]["subject"]) if parsed["prospect"]["subject"]
        end

        app_fields = ["classification", "research_performed", 
                    "action_id", "user_name", "channel_id", "status_code"]
        app_fields.each do |field|
          if parsed[field]
            event.set(field, parsed[field])
          end
        end
      end
    '
  }
    
  # Remove the parsed object after extracting data
  mutate { remove_field => ["parsed"] }
  }

  # Add service metadata
  if ![service][name] {
    mutate { add_field => { "[service][name]" => "agentic-sales-copilot" } }
  }
  if ![service][version] {
    mutate { add_field => { "[service][version]" => "0.1.0" } }
  }

  # Create conversation thread ID for email tracking
  if [prospect_email] and ![conversation][thread_id] {
    mutate {
      add_field => {
        "[conversation][thread_id]" => "%{[prospect_email]}|%{[subject]}"
      }
    }
  }

  # Rename fields to ECS-compliant structure
  mutate {
    rename => {
      "prospect_email" => "[prospect][email]"
      "subject" => "[prospect][subject]"
      "classification" => "[email][classification]"
      "research_performed" => "[email][research_performed]"
      "action_id" => "[slack][action_id]"
      "user_name" => "[slack][user]"
      "channel_id" => "[slack][channel_id]"
      "status_code" => "[http][response][status_code]"
    }
  }

  # Normalize boolean fields
  if [email][research_performed] {
    mutate {
      gsub => ["[email][research_performed]", "^(false|False)$", "false"]
      gsub => ["[email][research_performed]", "^(true|True)$", "true"]
    }
  }

  # Add timezone information
  if ![event][timezone_offset] {
    mutate { add_field => { "[event][timezone_offset]" => "+05:30" } }
  }

  # Content redaction for sensitive data
  ruby {
    code => '
      fields_to_mask = ["draft_reply","edited_text","body","conversation_history"]
      fields_to_mask.each do |f|
        if event.get(f)
          content = event.get(f).to_s
          # Store content length for analytics
          event.set("[content][#{f}][length]", content.length)
          # Redact the actual content
          event.set(f, "[REDACTED_CONTENT]")
        end
      end
    '
  }

  # Set error flag based on log level
  if [log][level] =~ /(?i)(ERROR|CRITICAL|FATAL)/ {
    mutate { add_field => { "is_error" => "true" } }
  } else {
    mutate { add_field => { "is_error" => "false" } }
  }

  # Convert the is_error field to a boolean type to ensure correct routing in the output block.
  mutate {
    convert => {
    "is_error" => "boolean"
    }
  }

  # Drop health check logs to reduce noise
  if [message] =~ "GET /health" {
    drop { }
  }

  # # Handle timestamp parsing
  # if ![@timestamp] and [asctime] {
  #   date {
  #     match => ["asctime","ISO8601","yyyy-MM-dd HH:mm:ss,SSS"]
  #     target => "@timestamp"
  #   }
  # }
  
  # # Ensure we always have a timestamp
  # if ![@timestamp] {
  #   mutate { add_field => { "@timestamp" => "%{+YYYY-MM-dd HH:mm:ss}" } }
  # }

  # Add processing metadata
  mutate {
    add_field => {
      "[event][ingested]" => "%{+YYYY-MM-dd HH:mm:ss}"
      "[agent][name]" => "logstash"
      "[agent][version]" => "8.11.3"
    }
  }
}

output {
  if [is_error] {
    # If the log is an error, send it to the error index.
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "agentic-sales-copilot-error-%{+YYYY.MM.dd}"
      retry_on_conflict => 3
      retry_max_interval => 5
      retry_initial_interval => 1
      pool_max => 1000
      pool_max_per_route => 100
      timeout => 60
      resurrect_delay => 5
      healthcheck_path => "/"
    }
  } else {
    # Otherwise, send it to the main index.
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "agentic-sales-copilot-%{+YYYY.MM.dd}"
      retry_on_conflict => 3
      retry_max_interval => 5
      retry_initial_interval => 1
      pool_max => 1000
      pool_max_per_route => 100
      timeout => 60
      resurrect_delay => 5
      healthcheck_path => "/"
    }
  }
}